{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(PROJECT_ROOT_DIR, \"images\", fig_id)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training = pickle.load(open('my_df_training_onehot.pickle', 'rb'))\n",
    "y = pickle.load(open('my_y.pickle', 'rb'))\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_training, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rui\\AppData\\Local\\conda\\conda\\envs\\myenv\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def tuned_clf(estimator,k,Xtrain,ytrain,pram_dist,ncv,njobs=1,scoring_method='None',**kwargs):\n",
    "    \"\"\"K: the number of features in SelectKBest method\n",
    "       Xtrain: training features\n",
    "       ytrain: traning labels \n",
    "       param dist: distribution parametes that are used in RandomizedSearchCV\n",
    "       ncv : number of cross-validation folds\n",
    "       This function will return the trained estimator\"\"\"\n",
    "    if kwargs:\n",
    "        clf = estimator(kwargs)\n",
    "    else:\n",
    "        clf = estimator\n",
    "    \n",
    "    if k == Xtrain.shape[1]:\n",
    "        pipe = make_pipeline(clf)\n",
    "    else:\n",
    "        pipe = make_pipeline(SelectKBest(k=k),clf)\n",
    "    \n",
    "    grid_clf = RandomizedSearchCV(pipe,param_distributions= param_dist,cv=ncv,n_jobs=njobs,scoring=scoring_method)\n",
    "    grid_clf.fit(Xtrain,ytrain) \n",
    "                                  \n",
    "    return  grid_clf\n",
    "\n",
    "def tuned_estimators(estimator,Xtrain,ytrain,Xtest,ytest,param_dist,n_features_list,ncv=5,njobs=1,scoring_method='None',\n",
    "                   verbose=False,**kwargs):\n",
    "    '''A kbest and a randomforestclassifier are embeded in a pipeline and a randomizedsearchCV tunes the\n",
    "     hyperparameters'''\n",
    "    models = defaultdict(str)\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "    roc_auc_scores = []\n",
    "    nfeatures = []\n",
    "    for k in n_features_list:\n",
    "        nfeatures.append(k)\n",
    "        model_name = 'clf_k'+ str(k)\n",
    "        clf = tuned_clf(estimator,k,Xtrain,ytrain,param_dist,ncv,njobs,scoring_method,**kwargs)\n",
    "        models[model_name] = clf.best_estimator_\n",
    "        ypred = models[model_name].predict(Xtest)\n",
    "        accuracy_scores.append (accuracy_score(ypred,ytest))\n",
    "        f1_scores.append(f1_score(ypred,ytest))\n",
    "        roc_auc_scores.append(roc_auc_score(ypred,ytest))\n",
    "        if verbose:\n",
    "            print('%s best features: accuracy=%.4f, f1=%.4f, roc_auc=%.4f' % \n",
    "                  (k,accuracy_scores[-1],f1_scores[-1],roc_auc_scores[-1]))\n",
    "    plt.title('Effect of feature elimination on accuracy, f1, roc_auc scores')\n",
    "    plt.xlabel(\"K best features\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(nfeatures)\n",
    "    plt.grid(b=True)\n",
    "    plt.plot(nfeatures,accuracy_scores,'o-', color=\"r\",label=\"accuracy score\")\n",
    "    plt.plot(nfeatures,f1_scores,'o-', color=\"b\",label=\"f1 score\")\n",
    "    plt.plot(nfeatures,roc_auc_scores,'o-', color=\"g\",label=\"roc_auc score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def Summary_Results(estimator,X_test,y_test):\n",
    "    ypred = estimator.predict(X_test)\n",
    "    print('The accuracy is: %.2f3 \\n' % accuracy_score(ypred,y_test))\n",
    "    print('Confusion_matrix:')\n",
    "    cm = confusion_matrix(y_test, ypred)\n",
    "    print('\\t\\t pridicted values')\n",
    "    print('\\t\\t 0 \\t 1')\n",
    "    print('actual 0: ','\\t',cm[0,0],'\\t',cm[0,1])\n",
    "    print('values 1: ','\\t',cm[1,0],'\\t',cm[1,1])\n",
    "    print('-------------------------------------------------------')\n",
    "    print('Classification_report: \\n')\n",
    "    print(classification_report(y_test,ypred,target_names=[\"class 0\",\"class 1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.88      0.99      0.05      0.93      0.54      0.31     16015\n",
      "          1       0.33      0.05      0.99      0.09      0.54      0.28      2231\n",
      "\n",
      "avg / total       0.81      0.87      0.16      0.83      0.54      0.31     18246\n",
      "\n",
      "RandomForest_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.88      0.99      0.03      0.93      0.56      0.34     16015\n",
      "          1       0.36      0.03      0.99      0.06      0.56      0.30      2231\n",
      "\n",
      "avg / total       0.82      0.87      0.15      0.83      0.56      0.33     18246\n",
      "\n",
      "AdaBoost_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.88      1.00      0.01      0.94      0.74      0.56     16015\n",
      "          1       0.62      0.01      1.00      0.03      0.74      0.53      2231\n",
      "\n",
      "avg / total       0.85      0.88      0.13      0.82      0.74      0.56     18246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from collections import defaultdict\n",
    "\n",
    "estimators = {'RandomForest':RandomForestClassifier(),'AdaBoost': AdaBoostClassifier(), 'Extra Tree': ExtraTreesClassifier()}\n",
    "clfs = defaultdict(str)\n",
    "\n",
    "for name,clf in estimators.items():\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(name + '_clf')\n",
    "    print(classification_report_imbalanced(y_test, y_pred))\n",
    "    clfs[name+'_clf'] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.88      0.97      0.09      0.93      0.52      0.28     16015\n",
      "          1       0.30      0.09      0.97      0.14      0.52      0.25      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.20      0.83      0.52      0.28     18246\n",
      "\n",
      "RandomForest_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.89      0.97      0.11      0.93      0.54      0.31     16015\n",
      "          1       0.33      0.11      0.97      0.16      0.54      0.27      2231\n",
      "\n",
      "avg / total       0.82      0.86      0.21      0.83      0.54      0.30     18246\n",
      "\n",
      "AdaBoost_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.89      0.91      0.21      0.90      0.47      0.23     16015\n",
      "          1       0.24      0.21      0.91      0.23      0.47      0.20      2231\n",
      "\n",
      "avg / total       0.81      0.82      0.30      0.82      0.47      0.23     18246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# oversample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sme = SMOTE()\n",
    "X_resampled, y_resampled = sme.fit_sample(X_train, y_train)\n",
    "for name,clf in estimators.items():\n",
    "    clf.fit(X_resampled,y_resampled)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(name + '_clf')\n",
    "    print(classification_report_imbalanced(y_test, y_pred))\n",
    "    clfs[name+'_SMOTEENN_clf'] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.91      0.80      0.43      0.85      0.46      0.22     16015\n",
      "          1       0.23      0.43      0.80      0.30      0.46      0.20      2231\n",
      "\n",
      "avg / total       0.83      0.75      0.48      0.78      0.46      0.22     18246\n",
      "\n",
      "RandomForest_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.91      0.79      0.44      0.85      0.46      0.22     16015\n",
      "          1       0.23      0.44      0.79      0.30      0.46      0.19      2231\n",
      "\n",
      "avg / total       0.83      0.75      0.48      0.78      0.46      0.22     18246\n",
      "\n",
      "AdaBoost_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.92      0.62      0.61      0.74      0.41      0.18     16015\n",
      "          1       0.18      0.61      0.62      0.28      0.41      0.16      2231\n",
      "\n",
      "avg / total       0.83      0.62      0.61      0.69      0.41      0.18     18246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine\n",
    "from imblearn.combine import SMOTEENN\n",
    "sme = SMOTEENN()\n",
    "X_resampled, y_resampled = sme.fit_sample(X_train, y_train)\n",
    "for name,clf in estimators.items():\n",
    "    clf.fit(X_resampled,y_resampled)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(name + '_clf')\n",
    "    print(classification_report_imbalanced(y_test, y_pred))\n",
    "    clfs[name+'_SMOTEENN_clf'] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.88      0.97      0.08      0.93      0.51      0.27     16015\n",
      "          1       0.29      0.08      0.97      0.13      0.51      0.24      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.19      0.83      0.51      0.27     18246\n",
      "\n",
      "RandomForest_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.88      0.97      0.08      0.93      0.50      0.26     16015\n",
      "          1       0.28      0.08      0.97      0.12      0.50      0.23      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.19      0.83      0.50      0.26     18246\n",
      "\n",
      "AdaBoost_clf\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.89      0.91      0.23      0.90      0.48      0.24     16015\n",
      "          1       0.25      0.23      0.91      0.24      0.48      0.21      2231\n",
      "\n",
      "avg / total       0.82      0.82      0.31      0.82      0.48      0.24     18246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine\n",
    "from imblearn.combine import SMOTETomek\n",
    "sme = SMOTETomek()\n",
    "X_resampled, y_resampled = sme.fit_sample(X_train, y_train)\n",
    "for name,clf in estimators.items():\n",
    "    clf.fit(X_resampled,y_resampled)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(name + '_clf')\n",
    "    print(classification_report_imbalanced(y_test, y_pred))\n",
    "    clfs[name+'_SMOTETomek_clf'] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest_SMOTEENN_clf:\n",
      "The accuracy is: 0.863 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 15560 \t 455\n",
      "values 1:  \t 2056 \t 175\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.88      0.97      0.93     16015\n",
      "    class 1       0.28      0.08      0.12      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.83     18246\n",
      "\n",
      "================================================================\n",
      "Extra Tree_clf:\n",
      "The accuracy is: 0.863 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 15555 \t 460\n",
      "values 1:  \t 2043 \t 188\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.88      0.97      0.93     16015\n",
      "    class 1       0.29      0.08      0.13      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.83     18246\n",
      "\n",
      "================================================================\n",
      "AdaBoost_SMOTETomek_clf:\n",
      "The accuracy is: 0.823 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 14533 \t 1482\n",
      "values 1:  \t 1726 \t 505\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.89      0.91      0.90     16015\n",
      "    class 1       0.25      0.23      0.24      2231\n",
      "\n",
      "avg / total       0.82      0.82      0.82     18246\n",
      "\n",
      "================================================================\n",
      "AdaBoost_SMOTEENN_clf:\n",
      "The accuracy is: 0.823 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 14533 \t 1482\n",
      "values 1:  \t 1726 \t 505\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.89      0.91      0.90     16015\n",
      "    class 1       0.25      0.23      0.24      2231\n",
      "\n",
      "avg / total       0.82      0.82      0.82     18246\n",
      "\n",
      "================================================================\n",
      "AdaBoost_clf:\n",
      "The accuracy is: 0.823 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 14533 \t 1482\n",
      "values 1:  \t 1726 \t 505\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.89      0.91      0.90     16015\n",
      "    class 1       0.25      0.23      0.24      2231\n",
      "\n",
      "avg / total       0.82      0.82      0.82     18246\n",
      "\n",
      "================================================================\n",
      "RandomForest_SMOTETomek_clf:\n",
      "The accuracy is: 0.863 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 15560 \t 455\n",
      "values 1:  \t 2056 \t 175\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.88      0.97      0.93     16015\n",
      "    class 1       0.28      0.08      0.12      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.83     18246\n",
      "\n",
      "================================================================\n",
      "Extra Tree_SMOTEENN_clf:\n",
      "The accuracy is: 0.863 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 15555 \t 460\n",
      "values 1:  \t 2043 \t 188\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.88      0.97      0.93     16015\n",
      "    class 1       0.29      0.08      0.13      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.83     18246\n",
      "\n",
      "================================================================\n",
      "RandomForest_clf:\n",
      "The accuracy is: 0.863 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 15560 \t 455\n",
      "values 1:  \t 2056 \t 175\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.88      0.97      0.93     16015\n",
      "    class 1       0.28      0.08      0.12      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.83     18246\n",
      "\n",
      "================================================================\n",
      "Extra Tree_SMOTETomek_clf:\n",
      "The accuracy is: 0.863 \n",
      "\n",
      "Confusion_matrix:\n",
      "\t\t pridicted values\n",
      "\t\t 0 \t 1\n",
      "actual 0:  \t 15555 \t 460\n",
      "values 1:  \t 2043 \t 188\n",
      "-------------------------------------------------------\n",
      "Classification_report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.88      0.97      0.93     16015\n",
      "    class 1       0.29      0.08      0.13      2231\n",
      "\n",
      "avg / total       0.81      0.86      0.83     18246\n",
      "\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "for name,clf in clfs.items():\n",
    "    print(name+':')\n",
    "    Summary_Results(clfs[name],X_test.as_matrix(),y_test.as_matrix())\n",
    "    print('================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import linear_model, decomposition\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_training, y)\n",
    "sme = SMOTEENN()\n",
    "X_train, y_train = sme.fit_sample(X_train, y_train)\n",
    "\n",
    "covar_matrix = PCA(n_components = 100)\n",
    "covar_matrix.fit(X_train)\n",
    "variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "\n",
    "var=np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3)*100)\n",
    "\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Features')\n",
    "plt.title('PCA Analysis')\n",
    "plt.ylim(30,100.5)\n",
    "plt.style.context('seaborn-whitegrid')\n",
    "\n",
    "plt.plot(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_components = 51\n",
    "\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "param_grid = {'C': [1e3, 1e4],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "clf = clf.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
